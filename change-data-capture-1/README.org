* What

This is a Proof-Of-Concept (POC) for using [[https://www.postgresql.org/docs/current/logicaldecoding.html][Logical Decoding]] in
PostgreSQL to perform [[https://en.wikipedia.org/wiki/Change_data_capture][Change Data Capture]] (CDC) and store the change
events back in the database.  The change events are then available for
Hasura [[https://hasura.io/docs/latest/subscriptions/postgres/index/][Subscriptions]]. 

* Why

Hasura users may need to update a `change` table that has a holistic
view of the changes made across many tables and then offer the ability
for other users to subscribe to those changes.

* How

This POC uses Logical Decoding along with the [[https://github.com/eulerto/wal2json][wal2json]] [[https://www.postgresql.org/docs/current/logicaldecoding-explanation.html#LOGICALDECODING-EXPLANATION-OUTPUT-PLUGINS][output plugin]]
and the [[https://github.com/citusdata/pg_cron][pg_cron]] [[https://www.postgresql.org/docs/current/sql-createextension.html][extension]].  Logical Decoding leverages the PostgreSQL
[[https://www.postgresql.org/docs/current/wal-intro.html][Write-Ahead Log]] (WAL) to support [[https://www.postgresql.org/docs/current/logical-replication.html][Logical Replication]] and other
applications.  Extra metadata in [[https://www.postgresql.org/docs/current/logicaldecoding-explanation.html#LOGICALDECODING-REPLICATION-SLOTS][replication slots]] causes PostgreSQL
to retain WAL segments instead of cleaning them up, processes change
events through custom output plugins to shape the data for downstream
applications, and offers applications the ability to control the
clean-up of WAL segments.

This POC uses Logical Decoding to capture /all/ changes to user
tables.  It uses the ~wal2json~ output plugin to shape the change
event data into a uniform JSON representation.  And, it uses the
~pg_cron~ extension along with PostgreSQL [[https://www.postgresql.org/docs/current/bgworker.html][background worker processes]]
to write the change event data in JSON form to a user ~change~ table
and to clean up WAL segments whose change event data has already been
processed in this way.

This POC uses a simple data model in [[file:initdb.d-postgres/01_init_model.sql][01_init_model.sql]] with sample
data in [[file:initdb.d-postgres/02_init_data.sql][02_init_data.sql]] for a retail grocery store, with these
tables.

- account :: user account data
- order :: order invoices that belong to an ~account~
- order_detail :: order line items that associate a ~product~ with an ~order~
- product :: a catalog of grocery items that can be part of an ~order~
- region :: sales regions associated with an ~order~

This POC sets up the CDC machinery in [[file:initdb.d-postgres/03_cdc.sql][03_cdc.sql]].  Among other things,
that creates a ~change~ table .  Finally, it has Hasura metadata in
the [[file:metadata/actions.graphql][metadata]] directory which tracks all of the user tables, including
the CDC ~change~ table.  That metadata is [[https://hasura.io/docs/latest/migrations-metadata-seeds/auto-apply-migrations/][automatically applied]] by the
Hasura Docker image.

* Steps

** Step 1:  Checkout the [[https://github.com/dventimihasura/hasura-projects][hasura-projects]] GitHub repository.

#+begin_src bash
  git clone https://github.com/dventimihasura/hasura-projects.git
#+end_src

** Step 2:  Change to the [[file:README.org][change-data-capture-1]] project directory.

#+begin_src bash
  cd hasura-projects/change-data-capture-1
#+end_src

** Step 3:  Create a ~.env~ file with suitable environment variable.

#+begin_src bash
 cat <<EOF > .env
 HGEPORT~<your exposed Hasura port>
 PGPORT~<your exposed PostgreSQL port>
 EOF
#+end_src

** Step 4:  Use [[https://docs.docker.com/compose/][Docker Compose]] to launch the services.

#+begin_src bash
  docker-compose up -d
#+end_src

or

#+begin_src
  docker compose up -d
#+end_src

** Step 5:  Use the [[https://hasura.io/docs/latest/hasura-cli/overview/][Hasura CLI]] to launch Console.

#+begin_src
  hasura console
#+end_src

** Step 6:  Create a [[https://hasura.io/docs/latest/subscriptions/postgres/streaming/index/][streaming subscription]] to the ~change~ top-level field.

#+begin_src graphql
  subscription {
    change_stream(cursor: {initial_value: {xid: 1}}, batch_size: 10) {
      lsn
      payload
      xid
    }
  }
#+end_src

** Step 7:  Use a SQL client like ~psql~ to make changes to user tables.

#+begin_src sql
  insert into "order" (account_id, status, region)
  values (
    (select id from account order by random() limit 1),
    'new',
    (select value from region order by random() limit 1));
#+end_src

** Step 8:  Observe subscription events as data are changed in user tables.

#+begin_src json
#+end_src

