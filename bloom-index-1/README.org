* What

Whis illustrates using Bloom indexes in PostgreSQL to speed up queries
with arbitrary combinations of predicates.

* Why

An optimal indexing strategy in databases like PostgreSQL will
typically be workload-dependent.  This can be challenging for online
applications with human operators given a free-form query interface
that offer arbitrary combinations of search filter predicates that
cannot be known in advance.  One way to meet this challenge is in
PostgreSQL is to use [[https://www.postgresql.org/docs/current/bloom.html][Bloom indexes]].

* How

This Proof-Of-Concept (POC) has these components.

- [[file:Dockerfile][Dockerfile]] :: This Dockerfile builds a custom Docker image for
  PostgreSQL 15 that includes the [[https://www.postgresql.org/docs/current/contrib.html][contrib]] package, which has the bloom
  index support.  It also adds to the image the [[https://faker.readthedocs.io/][Faker]] tool for
  generating fake sample data.
- [[file:docker-compose.yaml][docker-compose.yaml]] :: This Docker Compose file launches one service
  ~postgres~ whith initializes the database with the sample tables and
  data, and also loads the ~bloom~ shared library into PostgreSQL.
- [[file:initdb.d/01_init.sql][01_init.sql]] :: This SQL initialization script uses the Faker tool to
  build up fake sample data in a stepwise fashion in order to create a
  relatively large volume of data somewhat efficiently.  Currently, it
  builds up 1 billion fake ~profile~ records with these fields.
  - first_name
  - middle_name
  - last_name
  - city
  - color_name
  - job
  - company

* Steps

** Step 1:  Clone this repository.

#+begin_src bash
git clone https://github.com/dventimihasura/hasura-projects.git
#+end_src

** Step 2:  Change to the <> directory.

#+begin_src bash
cd bloom-index-1
#+end_src

** Step 3:  Launch the service with Docker Compose.

Note that this will take approximately an hour to synthesize all of
the fake sample data for the 1 billion sample rows

#+begin_src bash
docker compose up -d
#+end_src

** Step 4:  Connect a SQL client to the database.

#+begin_src bash
psql "postgres://postgres:postgres@localhost:15432/postgres"
#+end_src

** Step 5:  Execute the sample queries.

*** Select into a large table using the primary key.

The ~id~ column already has a unique B-tree index on it by virtue of
being a primary key.  This has maximum selectivity and has O(log n)
scaling.

#+begin_src sql :engine postgresql :dbhost localhost :dbuser postgres :dbpassword postgres :database postgres :dbport 15432 :results output :exports both
explain analyze
  with
  sample as (select id from profile tablesample system_rows(1))
select
  *
  from
    profile
    join sample on profile.id = sample.id;
#+end_src

#+RESULTS:
: QUERY PLAN
: Nested Loop  (cost=0.43..12.46 rows=1 width=130) (actual time=0.277..0.278 rows=1 loops=1)
:   ->  Sample Scan on profile profile_1  (cost=0.00..4.01 rows=1 width=16) (actual time=0.079..0.079 rows=1 loops=1)
:         Sampling: system_rows ('1'::bigint)
:   ->  Index Scan using profile_pkey on profile  (cost=0.43..8.45 rows=1 width=114) (actual time=0.195..0.195 rows=1 loops=1)
:         Index Cond: (id = profile_1.id)
: Planning Time: 0.397 ms
: Execution Time: 0.293 ms

*** Select into a large tables with a combination of predicates over non-primary key columns without the benefit of an index.

Without the benefit of an index the database will have to resort to a
sequential scan.  See the ~Seq Scan on profile~ output in the explain
plan.  This will scale as O(n) and execution times will be in seconds
or longer.

#+begin_src sql :engine postgresql :dbhost localhost :dbuser postgres :dbpassword postgres :database postgres :dbport 15432 :results output :exports both
explain analyze
with
  first_name as (select data from first_name order by random() limit 1),
  middle_name as (select data from middle_name order by random() limit 1),
  last_name as (select data from last_name order by random() limit 1),
  city as (select data from city order by random() limit 1),
  color_name as (select data from color_name order by random() limit 1),
  job as (select data from job order by random() limit 1),
  company as (select data from company order by random() limit 1)
select
  *
  from
    profile
    join first_name on profile.first_name = first_name.data
    join middle_name on profile.middle_name = middle_name.data
    join last_name on profile.last_name = last_name.data
    join city on profile.city = city.data
    join color_name on profile.color_name = color_name.data
    join job on profile.job = job.data
    join company on profile.company = company.data;
#+end_src

#+RESULTS:
#+begin_example
QUERY PLAN
Nested Loop  (cost=236.68..336745.82 rows=1 width=338) (actual time=1328.825..1742.717 rows=1 loops=1)
  Join Filter: (profile.company = company.data)
  Rows Removed by Join Filter: 9
  CTE first_name
    ->  Limit  (cost=33.80..33.80 rows=1 width=40) (actual time=0.006..0.008 rows=1 loops=1)
          ->  Sort  (cost=33.80..37.20 rows=1360 width=40) (actual time=0.005..0.007 rows=1 loops=1)
                Sort Key: (random())
                Sort Method: top-N heapsort  Memory: 25kB
                ->  Seq Scan on first_name first_name_1  (cost=0.00..27.00 rows=1360 width=40) (actual time=0.002..0.003 rows=10 loops=1)
  CTE middle_name
    ->  Limit  (cost=33.80..33.80 rows=1 width=40) (actual time=0.005..0.005 rows=1 loops=1)
          ->  Sort  (cost=33.80..37.20 rows=1360 width=40) (actual time=0.004..0.005 rows=1 loops=1)
                Sort Key: (random())
                Sort Method: top-N heapsort  Memory: 25kB
                ->  Seq Scan on middle_name middle_name_1  (cost=0.00..27.00 rows=1360 width=40) (actual time=0.002..0.003 rows=10 loops=1)
  CTE last_name
    ->  Limit  (cost=33.80..33.80 rows=1 width=40) (actual time=0.005..0.006 rows=1 loops=1)
          ->  Sort  (cost=33.80..37.20 rows=1360 width=40) (actual time=0.004..0.004 rows=1 loops=1)
                Sort Key: (random())
                Sort Method: top-N heapsort  Memory: 25kB
                ->  Seq Scan on last_name last_name_1  (cost=0.00..27.00 rows=1360 width=40) (actual time=0.002..0.003 rows=10 loops=1)
  CTE city
    ->  Limit  (cost=33.80..33.80 rows=1 width=40) (actual time=0.005..0.006 rows=1 loops=1)
          ->  Sort  (cost=33.80..37.20 rows=1360 width=40) (actual time=0.004..0.005 rows=1 loops=1)
                Sort Key: (random())
                Sort Method: top-N heapsort  Memory: 25kB
                ->  Seq Scan on city city_1  (cost=0.00..27.00 rows=1360 width=40) (actual time=0.002..0.003 rows=10 loops=1)
  CTE color_name
    ->  Limit  (cost=33.80..33.80 rows=1 width=40) (actual time=0.005..0.006 rows=1 loops=1)
          ->  Sort  (cost=33.80..37.20 rows=1360 width=40) (actual time=0.005..0.005 rows=1 loops=1)
                Sort Key: (random())
                Sort Method: top-N heapsort  Memory: 25kB
                ->  Seq Scan on color_name color_name_1  (cost=0.00..27.00 rows=1360 width=40) (actual time=0.002..0.003 rows=10 loops=1)
  CTE job
    ->  Limit  (cost=33.80..33.80 rows=1 width=40) (actual time=0.005..0.006 rows=1 loops=1)
          ->  Sort  (cost=33.80..37.20 rows=1360 width=40) (actual time=0.005..0.005 rows=1 loops=1)
                Sort Key: (random())
                Sort Method: top-N heapsort  Memory: 25kB
                ->  Seq Scan on job job_1  (cost=0.00..27.00 rows=1360 width=40) (actual time=0.002..0.003 rows=10 loops=1)
  CTE company
    ->  Limit  (cost=33.80..33.80 rows=1 width=40) (actual time=11.505..11.506 rows=1 loops=1)
          ->  Sort  (cost=33.80..37.20 rows=1360 width=40) (actual time=0.045..0.046 rows=1 loops=1)
                Sort Key: (random())
                Sort Method: top-N heapsort  Memory: 25kB
                ->  Seq Scan on company company_1  (cost=0.00..27.00 rows=1360 width=40) (actual time=0.025..0.026 rows=10 loops=1)
  ->  CTE Scan on company  (cost=0.00..0.02 rows=1 width=32) (actual time=11.507..11.508 rows=1 loops=1)
  ->  Nested Loop  (cost=0.07..336509.05 rows=10 width=306) (actual time=1317.313..1731.199 rows=10 loops=1)
        Join Filter: (profile.job = job.data)
        Rows Removed by Join Filter: 90
        ->  CTE Scan on job  (cost=0.00..0.02 rows=1 width=32) (actual time=0.006..0.007 rows=1 loops=1)
        ->  Nested Loop  (cost=0.07..336507.78 rows=100 width=274) (actual time=1317.275..1731.186 rows=100 loops=1)
              Join Filter: (profile.color_name = color_name.data)
              Rows Removed by Join Filter: 900
              ->  CTE Scan on color_name  (cost=0.00..0.02 rows=1 width=32) (actual time=0.007..0.008 rows=1 loops=1)
              ->  Nested Loop  (cost=0.07..336495.26 rows=1000 width=242) (actual time=1316.971..1731.139 rows=1000 loops=1)
                    Join Filter: (profile.city = city.data)
                    Rows Removed by Join Filter: 9000
                    ->  CTE Scan on city  (cost=0.00..0.02 rows=1 width=32) (actual time=0.006..0.006 rows=1 loops=1)
                    ->  Nested Loop  (cost=0.07..336370.23 rows=10001 width=210) (actual time=1313.029..1730.752 rows=10000 loops=1)
                          Join Filter: (profile.last_name = last_name.data)
                          Rows Removed by Join Filter: 90000
                          ->  CTE Scan on last_name  (cost=0.00..0.02 rows=1 width=32) (actual time=0.005..0.007 rows=1 loops=1)
                          ->  Hash Join  (cost=0.07..335120.06 rows=100012 width=178) (actual time=1308.970..1727.091 rows=100000 loops=1)
                                Hash Cond: (profile.middle_name = middle_name.data)
                                ->  Hash Join  (cost=0.03..330369.46 rows=1000118 width=146) (actual time=1092.109..1673.629 rows=1000000 loops=1)
                                      Hash Cond: (profile.first_name = first_name.data)
                                      ->  Seq Scan on profile  (cost=0.00..282863.82 rows=10001182 width=114) (actual time=0.086..1046.735 rows=10000000 loops=1)
                                      ->  Hash  (cost=0.02..0.02 rows=1 width=32) (actual time=0.007..0.008 rows=1 loops=1)
                                            Buckets: 1024  Batches: 1  Memory Usage: 9kB
                                            ->  CTE Scan on first_name  (cost=0.00..0.02 rows=1 width=32) (actual time=0.007..0.007 rows=1 loops=1)
                                ->  Hash  (cost=0.02..0.02 rows=1 width=32) (actual time=0.009..0.009 rows=1 loops=1)
                                      Buckets: 1024  Batches: 1  Memory Usage: 9kB
                                      ->  CTE Scan on middle_name  (cost=0.00..0.02 rows=1 width=32) (actual time=0.007..0.007 rows=1 loops=1)
Planning Time: 1.142 ms
JIT:
  Functions: 60
  Options: Inlining false, Optimization false, Expressions true, Deforming true
  Timing: Generation 2.045 ms, Inlining 0.000 ms, Optimization 0.530 ms, Emission 10.761 ms, Total 13.336 ms
Execution Time: 1762.171 ms
#+end_example

*** Add a bloom index over all the non-primary key columns in the ~profile~ table.

#+begin_src sql :engine postgresql :dbhost localhost :dbuser postgres :dbpassword postgres :database postgres :dbport 15432 :results output :exports both
create index bloomidx on profile using bloom(first_name, middle_name, last_name, city, color_name, job, company, license_plate);
#+end_src

#+RESULTS:
: CREATE INDEX

*** Select into a large tables with a combination of predicates over non-primary key columns /with/ the benefit of an index.

With the benefit of an index, the database can avoid a sequential
scane.  Note the ~Bitmap Index Scan on bloomidx~ in the explain plan.
This will scale as O(log n) and have execution times in milliseconds.

#+begin_src sql :engine postgresql :dbhost localhost :dbuser postgres :dbpassword postgres :database postgres :dbport 15432 :results output :exports both
explain analyze
with
  first_name as (select data from first_name order by random() limit 1),
  middle_name as (select data from middle_name order by random() limit 1),
  last_name as (select data from last_name order by random() limit 1),
  city as (select data from city order by random() limit 1),
  color_name as (select data from color_name order by random() limit 1),
  job as (select data from job order by random() limit 1),
  company as (select data from company order by random() limit 1)
select
  *
  from
    profile
    join first_name on profile.first_name = first_name.data
    join middle_name on profile.middle_name = middle_name.data
    join last_name on profile.last_name = last_name.data
    join city on profile.city = city.data
    join color_name on profile.color_name = color_name.data
    join job on profile.job = job.data
    join company on profile.company = company.data;
#+end_src

#+RESULTS:
#+begin_example
QUERY PLAN
Hash Join  (cost=228672.96..232509.13 rows=1 width=338) (actual time=44.208..46.754 rows=1 loops=1)
  Hash Cond: (profile.company = company.data)
  CTE first_name
    ->  Limit  (cost=33.80..33.80 rows=1 width=40) (actual time=0.005..0.006 rows=1 loops=1)
          ->  Sort  (cost=33.80..37.20 rows=1360 width=40) (actual time=0.004..0.005 rows=1 loops=1)
                Sort Key: (random())
                Sort Method: top-N heapsort  Memory: 25kB
                ->  Seq Scan on first_name first_name_1  (cost=0.00..27.00 rows=1360 width=40) (actual time=0.002..0.003 rows=10 loops=1)
  CTE middle_name
    ->  Limit  (cost=33.80..33.80 rows=1 width=40) (actual time=0.005..0.006 rows=1 loops=1)
          ->  Sort  (cost=33.80..37.20 rows=1360 width=40) (actual time=0.005..0.005 rows=1 loops=1)
                Sort Key: (random())
                Sort Method: top-N heapsort  Memory: 25kB
                ->  Seq Scan on middle_name middle_name_1  (cost=0.00..27.00 rows=1360 width=40) (actual time=0.002..0.003 rows=10 loops=1)
  CTE last_name
    ->  Limit  (cost=33.80..33.80 rows=1 width=40) (actual time=0.005..0.006 rows=1 loops=1)
          ->  Sort  (cost=33.80..37.20 rows=1360 width=40) (actual time=0.005..0.005 rows=1 loops=1)
                Sort Key: (random())
                Sort Method: top-N heapsort  Memory: 25kB
                ->  Seq Scan on last_name last_name_1  (cost=0.00..27.00 rows=1360 width=40) (actual time=0.003..0.003 rows=10 loops=1)
  CTE city
    ->  Limit  (cost=33.80..33.80 rows=1 width=40) (actual time=0.005..0.006 rows=1 loops=1)
          ->  Sort  (cost=33.80..37.20 rows=1360 width=40) (actual time=0.004..0.005 rows=1 loops=1)
                Sort Key: (random())
                Sort Method: top-N heapsort  Memory: 25kB
                ->  Seq Scan on city city_1  (cost=0.00..27.00 rows=1360 width=40) (actual time=0.002..0.003 rows=10 loops=1)
  CTE color_name
    ->  Limit  (cost=33.80..33.80 rows=1 width=40) (actual time=0.005..0.006 rows=1 loops=1)
          ->  Sort  (cost=33.80..37.20 rows=1360 width=40) (actual time=0.005..0.005 rows=1 loops=1)
                Sort Key: (random())
                Sort Method: top-N heapsort  Memory: 25kB
                ->  Seq Scan on color_name color_name_1  (cost=0.00..27.00 rows=1360 width=40) (actual time=0.003..0.003 rows=10 loops=1)
  CTE job
    ->  Limit  (cost=33.80..33.80 rows=1 width=40) (actual time=0.006..0.007 rows=1 loops=1)
          ->  Sort  (cost=33.80..37.20 rows=1360 width=40) (actual time=0.005..0.006 rows=1 loops=1)
                Sort Key: (random())
                Sort Method: top-N heapsort  Memory: 25kB
                ->  Seq Scan on job job_1  (cost=0.00..27.00 rows=1360 width=40) (actual time=0.003..0.004 rows=10 loops=1)
  CTE company
    ->  Limit  (cost=33.80..33.80 rows=1 width=40) (actual time=12.251..12.251 rows=1 loops=1)
          ->  Sort  (cost=33.80..37.20 rows=1360 width=40) (actual time=0.044..0.044 rows=1 loops=1)
                Sort Key: (random())
                Sort Method: top-N heapsort  Memory: 25kB
                ->  Seq Scan on company company_1  (cost=0.00..27.00 rows=1360 width=40) (actual time=0.024..0.025 rows=10 loops=1)
  ->  Hash Join  (cost=228436.32..232272.43 rows=10 width=306) (actual time=31.937..34.480 rows=10 loops=1)
        Hash Cond: (profile.job = job.data)
        ->  Hash Join  (cost=228436.28..232271.92 rows=100 width=274) (actual time=31.893..34.461 rows=100 loops=1)
              Hash Cond: (profile.color_name = color_name.data)
              ->  Nested Loop  (cost=228436.25..232267.14 rows=1000 width=242) (actual time=31.598..34.396 rows=1000 loops=1)
                    ->  CTE Scan on city  (cost=0.00..0.02 rows=1 width=32) (actual time=0.006..0.007 rows=1 loops=1)
                    ->  Nested Loop  (cost=228436.25..232257.12 rows=1000 width=210) (actual time=31.590..34.338 rows=1000 loops=1)
                          ->  CTE Scan on last_name  (cost=0.00..0.02 rows=1 width=32) (actual time=0.006..0.007 rows=1 loops=1)
                          ->  Nested Loop  (cost=228436.25..232247.10 rows=1000 width=178) (actual time=31.583..34.283 rows=1000 loops=1)
                                ->  CTE Scan on middle_name  (cost=0.00..0.02 rows=1 width=32) (actual time=0.005..0.006 rows=1 loops=1)
                                ->  Nested Loop  (cost=228436.25..232237.08 rows=1000 width=146) (actual time=31.577..34.228 rows=1000 loops=1)
                                      ->  CTE Scan on first_name  (cost=0.00..0.02 rows=1 width=32) (actual time=0.006..0.007 rows=1 loops=1)
                                      ->  Bitmap Heap Scan on profile  (cost=228436.25..232227.06 rows=1000 width=114) (actual time=31.567..34.149 rows=1000 loops=1)
                                            Recheck Cond: ((first_name = first_name.data) AND (middle_name = middle_name.data) AND (last_name = last_name.data) AND (city = city.data))
                                            Rows Removed by Index Recheck: 580
                                            Heap Blocks: exact=174
                                            ->  Bitmap Index Scan on bloomidx  (cost=0.00..228436.00 rows=1000 width=0) (actual time=30.924..30.924 rows=1580 loops=1)
                                                  Index Cond: ((first_name = first_name.data) AND (middle_name = middle_name.data) AND (last_name = last_name.data) AND (city = city.data))
              ->  Hash  (cost=0.02..0.02 rows=1 width=32) (actual time=0.006..0.007 rows=1 loops=1)
                    Buckets: 1024  Batches: 1  Memory Usage: 9kB
                    ->  CTE Scan on color_name  (cost=0.00..0.02 rows=1 width=32) (actual time=0.006..0.006 rows=1 loops=1)
        ->  Hash  (cost=0.02..0.02 rows=1 width=32) (actual time=0.007..0.008 rows=1 loops=1)
              Buckets: 1024  Batches: 1  Memory Usage: 9kB
              ->  CTE Scan on job  (cost=0.00..0.02 rows=1 width=32) (actual time=0.007..0.007 rows=1 loops=1)
  ->  Hash  (cost=0.02..0.02 rows=1 width=32) (actual time=12.256..12.256 rows=1 loops=1)
        Buckets: 1024  Batches: 1  Memory Usage: 9kB
        ->  CTE Scan on company  (cost=0.00..0.02 rows=1 width=32) (actual time=12.254..12.254 rows=1 loops=1)
Planning Time: 2.816 ms
JIT:
  Functions: 64
  Options: Inlining false, Optimization false, Expressions true, Deforming true
  Timing: Generation 2.147 ms, Inlining 0.000 ms, Optimization 0.564 ms, Emission 11.460 ms, Total 14.171 ms
Execution Time: 69.388 ms
#+end_example
