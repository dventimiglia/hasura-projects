* What

This illustrates using Bloom indexes in PostgreSQL to speed up queries
with arbitrary combinations of predicates.

* Why

An optimal indexing strategy in databases like PostgreSQL will
typically be workload-dependent.  This can be challenging for online
applications with human operators given a free-form query interface
that offer arbitrary combinations of search filter predicates that
cannot be known in advance.  One way to meet this challenge is in
PostgreSQL is to use [[https://www.postgresql.org/docs/current/bloom.html][Bloom indexes]].

* How

This Proof-Of-Concept (POC) has these components.

- [[file:Dockerfile][Dockerfile]] :: This Dockerfile builds a custom Docker image for
  PostgreSQL 15 that includes the [[https://www.postgresql.org/docs/current/contrib.html][contrib]] package, which has the bloom
  index support.  It also adds to the image the [[https://faker.readthedocs.io/][Faker]] tool for
  generating fake sample data.
- [[file:docker-compose.yaml][docker-compose.yaml]] :: This Docker Compose file launches one service
  ~postgres~ whith initializes the database with the sample tables and
  data, and also loads the ~bloom~ shared library into PostgreSQL.
- [[file:initdb.d/01_init.sql][01_init.sql]] :: This SQL initialization script uses the Faker tool to
  build up fake sample data in a stepwise fashion in order to create a
  relatively large volume of data somewhat efficiently.  Currently, it
  builds up 100 million fake ~profile~ records with these fields.
  - first_name
  - middle_name
  - last_name
  - city
  - color_name
  - job
  - company
  - license_plate

* Steps

** Step 1:  Clone this repository.

#+begin_src bash
git clone https://github.com/dventimihasura/hasura-projects.git
#+end_src

** Step 2:  Change to the ~bloom-index-1~ directory.

#+begin_src bash
cd bloom-index-1
#+end_src

** Step 3:  Launch the service with Docker Compose.

Note that this will take approximately an hour to synthesize all of
the fake sample data for the 100 million sample rows

#+begin_src bash
docker compose up -d
#+end_src

** Step 4:  Connect a SQL client to the database.

#+begin_src bash
psql "postgres://postgres:postgres@localhost:15432/postgres"
#+end_src

** Step 5:  Execute the sample queries.

*** Verify the volume of data.

#+begin_src sql :engine postgresql :dbhost localhost :dbuser postgres :dbpassword postgres :database postgres :dbport 15432 :results output :exports both
select count(1) from profile;
#+end_src

#+RESULTS:
: count
: 100000000

*** Select into a large table using the primary key.

The ~id~ column already has a unique B-tree index on it by virtue of
being a primary key.  This has maximum selectivity and has O(log n)
scaling.

#+begin_src sql :engine postgresql :dbhost localhost :dbuser postgres :dbpassword postgres :database postgres :dbport 15432 :results output :exports both
explain analyze
with
  sample as (select id from profile tablesample system_rows(1))
select
  *
  from
    profile
    join sample on profile.id = sample.id;
#+end_src

#+RESULTS:
: QUERY PLAN
: Nested Loop  (cost=0.57..12.60 rows=1 width=130) (actual time=0.325..0.326 rows=1 loops=1)
:   ->  Sample Scan on profile profile_1  (cost=0.00..4.01 rows=1 width=16) (actual time=0.075..0.075 rows=1 loops=1)
:         Sampling: system_rows ('1'::bigint)
:   ->  Index Scan using profile_pkey on profile  (cost=0.57..8.59 rows=1 width=114) (actual time=0.247..0.247 rows=1 loops=1)
:         Index Cond: (id = profile_1.id)
: Planning Time: 1.244 ms
: Execution Time: 0.346 ms

*** Select into a large tables with a combination of predicates over non-primary key columns without the benefit of an index.

Without the benefit of an index the database will have to resort to a
sequential scan.  See the ~Seq Scan on profile~ output in the explain
plan.  This will scale as O(n) and execution times will be in seconds
or longer.

#+begin_src sql :engine postgresql :dbhost localhost :dbuser postgres :dbpassword postgres :database postgres :dbport 15432 :results output :exports both
explain analyze
with
  first_name as (select data from first_name tablesample system_rows(1)),
  middle_name as (select data from middle_name tablesample system_rows(1)),
  last_name as (select data from last_name tablesample system_rows(1)),
  city as (select data from city tablesample system_rows(1)),
  color_name as (select data from color_name tablesample system_rows(1)),
  job as (select data from job tablesample system_rows(1)),
  company as (select data from company tablesample system_rows(1)),
  license_plate as (select data from license_plate tablesample system_rows(1))
select
  count(1)
  from
    profile
    join first_name on profile.first_name = first_name.data
    join middle_name on profile.middle_name = middle_name.data
    join last_name on profile.last_name = last_name.data
    join city on profile.city = city.data
    join color_name on profile.color_name = color_name.data
    join job on profile.job = job.data
    join company on profile.company = company.data
    join license_plate on profile.license_plate = license_plate.data;
#+end_src

#+RESULTS:
#+begin_example
QUERY PLAN
Aggregate  (cost=3377187.99..3377188.00 rows=1 width=8) (actual time=20119.543..20119.548 rows=1 loops=1)
  ->  Nested Loop  (cost=8.04..3377187.99 rows=1 width=0) (actual time=4041.894..20119.531 rows=1 loops=1)
        Join Filter: (profile.license_plate = license_plate.data)
        Rows Removed by Join Filter: 9
        ->  Sample Scan on license_plate  (cost=0.00..4.01 rows=1 width=32) (actual time=142.236..142.238 rows=1 loops=1)
              Sampling: system_rows ('1'::bigint)
        ->  Nested Loop  (cost=8.04..3377183.84 rows=11 width=9) (actual time=0.065..19977.275 rows=10 loops=1)
              Join Filter: (profile.job = job.data)
              Rows Removed by Join Filter: 90
              ->  Sample Scan on job  (cost=0.00..4.01 rows=1 width=32) (actual time=0.004..0.005 rows=1 loops=1)
                    Sampling: system_rows ('1'::bigint)
              ->  Nested Loop  (cost=8.04..3377178.58 rows=100 width=33) (actual time=0.060..19977.235 rows=100 loops=1)
                    Join Filter: (profile.company = company.data)
                    Rows Removed by Join Filter: 900
                    ->  Sample Scan on company  (cost=0.00..4.01 rows=1 width=32) (actual time=0.003..0.003 rows=1 loops=1)
                          Sampling: system_rows ('1'::bigint)
                    ->  Nested Loop  (cost=8.04..3377162.07 rows=1000 width=53) (actual time=0.056..19977.185 rows=1000 loops=1)
                          Join Filter: (profile.color_name = color_name.data)
                          Rows Removed by Join Filter: 9000
                          ->  Sample Scan on color_name  (cost=0.00..4.01 rows=1 width=32) (actual time=0.002..0.003 rows=1 loops=1)
                                Sampling: system_rows ('1'::bigint)
                          ->  Nested Loop  (cost=8.04..3377033.06 rows=10000 width=63) (actual time=0.052..19976.655 rows=10000 loops=1)
                                Join Filter: (profile.city = city.data)
                                Rows Removed by Join Filter: 90000
                                ->  Sample Scan on city  (cost=0.00..4.01 rows=1 width=32) (actual time=0.004..0.004 rows=1 loops=1)
                                      Sampling: system_rows ('1'::bigint)
                                ->  Nested Loop  (cost=8.04..3375779.05 rows=100000 width=75) (actual time=0.047..19972.914 rows=100000 loops=1)
                                      Join Filter: (profile.last_name = last_name.data)
                                      Rows Removed by Join Filter: 900000
                                      ->  Sample Scan on last_name  (cost=0.00..4.01 rows=1 width=32) (actual time=0.002..0.004 rows=1 loops=1)
                                            Sampling: system_rows ('1'::bigint)
                                      ->  Hash Join  (cost=8.04..3363275.04 rows=1000000 width=83) (actual time=0.042..19933.145 rows=1000000 loops=1)
                                            Hash Cond: (profile.middle_name = middle_name.data)
                                            ->  Hash Join  (cost=4.02..3315771.02 rows=10000000 width=90) (actual time=0.026..19366.723 rows=10000000 loops=1)
                                                  Hash Cond: (profile.first_name = first_name.data)
                                                  ->  Seq Scan on profile  (cost=0.00..2840767.00 rows=100000000 width=98) (actual time=0.015..13162.457 rows=100000000 loops=1)
                                                  ->  Hash  (cost=4.01..4.01 rows=1 width=32) (actual time=0.007..0.008 rows=1 loops=1)
                                                        Buckets: 1024  Batches: 1  Memory Usage: 9kB
                                                        ->  Sample Scan on first_name  (cost=0.00..4.01 rows=1 width=32) (actual time=0.006..0.006 rows=1 loops=1)
                                                              Sampling: system_rows ('1'::bigint)
                                            ->  Hash  (cost=4.01..4.01 rows=1 width=32) (actual time=0.006..0.007 rows=1 loops=1)
                                                  Buckets: 1024  Batches: 1  Memory Usage: 9kB
                                                  ->  Sample Scan on middle_name  (cost=0.00..4.01 rows=1 width=32) (actual time=0.003..0.003 rows=1 loops=1)
                                                        Sampling: system_rows ('1'::bigint)
Planning Time: 1.440 ms
JIT:
  Functions: 43
  Options: Inlining true, Optimization true, Expressions true, Deforming true
  Timing: Generation 1.429 ms, Inlining 31.527 ms, Optimization 67.497 ms, Emission 43.006 ms, Total 143.459 ms
Execution Time: 20139.044 ms
#+end_example

*** Add a bloom index over all the non-primary key columns in the ~profile~ table.

#+begin_src sql :engine postgresql :dbhost localhost :dbuser postgres :dbpassword postgres :database postgres :dbport 15432 :results output :exports both
create index bloomidx on profile using bloom(first_name, middle_name, last_name, city, color_name, job, company, license_plate);
#+end_src

#+RESULTS:
: CREATE INDEX

*** Select into a large tables with a combination of predicates over non-primary key columns /with/ the benefit of an index.

With the benefit of an index, the database can avoid a sequential
scane.  Note the ~Bitmap Index Scan on bloomidx~ in the explain plan.
This will scale as O(1) and have execution times in milliseconds.

#+begin_src sql :engine postgresql :dbhost localhost :dbuser postgres :dbpassword postgres :database postgres :dbport 15432 :results output :exports both
explain analyze
with
  first_name as (select data from first_name tablesample system_rows(1)),
  middle_name as (select data from middle_name tablesample system_rows(1)),
  last_name as (select data from last_name tablesample system_rows(1)),
  city as (select data from city tablesample system_rows(1)),
  color_name as (select data from color_name tablesample system_rows(1)),
  job as (select data from job tablesample system_rows(1)),
  company as (select data from company tablesample system_rows(1)),
  license_plate as (select data from license_plate tablesample system_rows(1))
select
  count(1)
  from
    profile
    join first_name on profile.first_name = first_name.data
    join middle_name on profile.middle_name = middle_name.data
    join last_name on profile.last_name = last_name.data
    join city on profile.city = city.data
    join color_name on profile.color_name = color_name.data
    join job on profile.job = job.data
    join company on profile.company = company.data
    join license_plate on profile.license_plate = license_plate.data;
#+end_src

#+RESULTS:
#+begin_example
QUERY PLAN
Aggregate  (cost=2322697.20..2322697.21 rows=1 width=8) (actual time=1457.889..1457.895 rows=1 loops=1)
  ->  Hash Join  (cost=2284338.59..2322697.20 rows=1 width=0) (actual time=1453.215..1457.889 rows=1 loops=1)
        Hash Cond: (profile.license_plate = license_plate.data)
        ->  Hash Join  (cost=2284334.57..2322693.12 rows=11 width=9) (actual time=1309.489..1315.134 rows=10 loops=1)
              Hash Cond: (profile.job = job.data)
              ->  Hash Join  (cost=2284330.54..2322688.62 rows=100 width=33) (actual time=1309.471..1315.107 rows=100 loops=1)
                    Hash Cond: (profile.company = company.data)
                    ->  Hash Join  (cost=2284326.52..2322679.84 rows=1000 width=53) (actual time=1309.464..1315.031 rows=1000 loops=1)
                          Hash Cond: (profile.color_name = color_name.data)
                          ->  Nested Loop  (cost=2284322.50..2322628.32 rows=10000 width=63) (actual time=1309.454..1314.522 rows=10000 loops=1)
                                ->  Sample Scan on city  (cost=0.00..4.01 rows=1 width=32) (actual time=0.002..0.004 rows=1 loops=1)
                                      Sampling: system_rows ('1'::bigint)
                                ->  Nested Loop  (cost=2284322.50..2322524.31 rows=10000 width=75) (actual time=1309.446..1314.085 rows=10000 loops=1)
                                      ->  Sample Scan on last_name  (cost=0.00..4.01 rows=1 width=32) (actual time=0.003..0.004 rows=1 loops=1)
                                            Sampling: system_rows ('1'::bigint)
                                      ->  Nested Loop  (cost=2284322.50..2322420.30 rows=10000 width=83) (actual time=1309.442..1313.651 rows=10000 loops=1)
                                            ->  Sample Scan on middle_name  (cost=0.00..4.01 rows=1 width=32) (actual time=0.002..0.003 rows=1 loops=1)
                                                  Sampling: system_rows ('1'::bigint)
                                            ->  Nested Loop  (cost=2284322.50..2322316.29 rows=10000 width=90) (actual time=1309.438..1313.209 rows=10000 loops=1)
                                                  ->  Sample Scan on first_name  (cost=0.00..4.01 rows=1 width=32) (actual time=0.008..0.010 rows=1 loops=1)
                                                        Sampling: system_rows ('1'::bigint)
                                                  ->  Bitmap Heap Scan on profile  (cost=2284322.50..2322212.28 rows=10000 width=98) (actual time=1309.425..1312.525 rows=10000 loops=1)
                                                        Recheck Cond: ((first_name = first_name.data) AND (middle_name = middle_name.data) AND (last_name = last_name.data) AND (city = city.data))
                                                        Rows Removed by Index Recheck: 6600
                                                        Heap Blocks: exact=1445
                                                        ->  Bitmap Index Scan on bloomidx  (cost=0.00..2284320.00 rows=10000 width=0) (actual time=1309.276..1309.277 rows=16600 loops=1)
                                                              Index Cond: ((first_name = first_name.data) AND (middle_name = middle_name.data) AND (last_name = last_name.data) AND (city = city.data))
                          ->  Hash  (cost=4.01..4.01 rows=1 width=32) (actual time=0.005..0.006 rows=1 loops=1)
                                Buckets: 1024  Batches: 1  Memory Usage: 9kB
                                ->  Sample Scan on color_name  (cost=0.00..4.01 rows=1 width=32) (actual time=0.004..0.004 rows=1 loops=1)
                                      Sampling: system_rows ('1'::bigint)
                    ->  Hash  (cost=4.01..4.01 rows=1 width=32) (actual time=0.004..0.004 rows=1 loops=1)
                          Buckets: 1024  Batches: 1  Memory Usage: 9kB
                          ->  Sample Scan on company  (cost=0.00..4.01 rows=1 width=32) (actual time=0.003..0.003 rows=1 loops=1)
                                Sampling: system_rows ('1'::bigint)
              ->  Hash  (cost=4.01..4.01 rows=1 width=32) (actual time=0.008..0.008 rows=1 loops=1)
                    Buckets: 1024  Batches: 1  Memory Usage: 9kB
                    ->  Sample Scan on job  (cost=0.00..4.01 rows=1 width=32) (actual time=0.005..0.006 rows=1 loops=1)
                          Sampling: system_rows ('1'::bigint)
        ->  Hash  (cost=4.01..4.01 rows=1 width=32) (actual time=142.740..142.741 rows=1 loops=1)
              Buckets: 1024  Batches: 1  Memory Usage: 9kB
              ->  Sample Scan on license_plate  (cost=0.00..4.01 rows=1 width=32) (actual time=142.729..142.730 rows=1 loops=1)
                    Sampling: system_rows ('1'::bigint)
Planning Time: 2.838 ms
JIT:
  Functions: 45
  Options: Inlining true, Optimization true, Expressions true, Deforming true
  Timing: Generation 1.526 ms, Inlining 32.610 ms, Optimization 64.802 ms, Emission 45.080 ms, Total 144.017 ms
Execution Time: 1477.340 ms
#+end_example
